---
# Cleanup tasks - these run at the beginning to ensure clean state
- name: Clean up previous installation directories
  file:
    path: "{{ item }}"
    state: absent
  loop:
    - "{{ ansible_env.HOME }}/tdx"
    - "{{ ansible_env.HOME }}/operator"
    - "{{ ansible_env.HOME }}/trustee"
  become: true
  tags:
    - cleanup
    - always

# Configure Intel TDX on host
- name: Update and install required packages
  apt:
    name:
      - qemu-system-x86
      - libvirt-daemon-system
      - build-essential
      - git
    update_cache: yes
    state: present
  become: true

# Create fake containerd service before Kubernetes tasks
- name: Create fake containerd script
  copy:
    content: |
      #!/bin/bash
      echo "Fake containerd is running..."
      sleep infinity
    dest: /usr/local/bin/fake-containerd
    mode: "0755"
  become: true

- name: Create fake containerd service file
  copy:
    content: |
      [Unit]
      Description=Fake containerd service
      After=network.target

      [Service]
      ExecStart=/usr/local/bin/fake-containerd
      Restart=always

      [Install]
      WantedBy=multi-user.target
    dest: /etc/systemd/system/containerd.service
  become: true

- name: Reload systemd daemon
  systemd:
    daemon_reexec: yes
    daemon_reload: yes
  become: true

- name: Enable and start fake containerd service
  systemd:
    name: containerd
    enabled: yes
    state: started
  become: true

- name: Check fake containerd service status
  systemd:
    name: containerd
  register: containerd_status
  become: true

- name: Display containerd service status
  debug:
    msg: "Containerd service is {{ containerd_status.status.ActiveState }}"

# First check and clean up if needed
- name: Check if TDX directory exists with local modifications
  stat:
    path: "{{ ansible_env.HOME }}/tdx/.git"
  register: tdx_git_dir

- name: Clean up existing TDX repository if needed
  file:
    path: "{{ ansible_env.HOME }}/tdx"
    state: absent
  when: tdx_git_dir.stat.exists
  become: true

# Now clone fresh
- name: Clone Canonical TDX repository
  git:
    repo: https://github.com/canonical/tdx.git
    dest: "{{ ansible_env.HOME }}/tdx"
    version: main

- name: Create TDX config directory
  file:
    path: "{{ ansible_env.HOME }}/tdx/config"
    state: directory

- name: Copy TDX configuration
  template:
    src: setup-tdx-config.j2
    dest: "{{ ansible_env.HOME }}/tdx/setup-tdx-config"

# Add debug steps to check repository contents
- name: List repository contents
  shell: ls -la {{ ansible_env.HOME }}/tdx/
  register: repo_contents
  become: true

- name: Debug repository contents
  debug:
    var: repo_contents.stdout_lines
  become: true

# Make sure the script is executable
- name: Ensure setup script is executable
  file:
    path: "{{ ansible_env.HOME }}/tdx/setup-tdx-host.sh"
    mode: "0755"
  become: true

- name: Setup TDX host (Intel TDX-enabled host OS)
  shell: |
    cd {{ ansible_env.HOME }}/tdx
    ./setup-tdx-host.sh
  become: true

# Configure Kubernetes node for CoCo
- name: Label node for CoCo operator
  shell: kubectl label node {{ inventory_hostname }} "node-role.kubernetes.io/worker=" --overwrite
  args:
    executable: /bin/bash
  when:
    - inventory_hostname in groups['worker_nodes'] or (groups['all'] | length == 1)
  delegate_to: "{{ groups['master_nodes'][0] }}"
  ignore_errors: yes

# Install Confidential Containers Operator (master nodes only or single node)
- name: Clone Confidential Containers Operator repository
  git:
    repo: https://github.com/confidential-containers/operator
    dest: "{{ ansible_env.HOME }}/operator"
    version: main
  when: inventory_hostname in groups['master_nodes']

- name: Apply CoCo operator from local repository
  shell: |
    cd {{ ansible_env.HOME }}/operator
    kubectl apply -k config/default
  args:
    executable: /bin/bash
  when: inventory_hostname in groups['master_nodes']

- name: Wait for CoCo operator deployment to be available
  shell: kubectl wait -n confidential-containers-system --for=condition=available --timeout=5m deployment/cc-operator-controller-manager
  args:
    executable: /bin/bash
  when: inventory_hostname in groups['master_nodes']

# Create Rancher customization
- name: Create Rancher directory for CoCo operator
  file:
    path: "{{ ansible_env.HOME }}/operator/config/samples/ccruntime/rancher"
    state: directory
  when: inventory_hostname in groups['master_nodes']

- name: Create Rancher kustomization file
  copy:
    content: |
      apiVersion: kustomize.config.k8s.io/v1beta1
      kind: Kustomization

      nameSuffix: -rke2

      resources:
      - ../base

      images:
      - name: quay.io/confidential-containers/reqs-payload
        newTag: latest
      - name: quay.io/kata-containers/kata-deploy
        newName: quay.io/kata-containers/kata-deploy-ci
        newTag: kata-containers-latest

      patches:
      - patch: |-
          - op: replace
            path: /spec/config/runtimeClasses
            value:
            - name: "kata-clh"
              snapshotter: ""
              pulltype: ""
            - name: "kata-qemu"
              snapshotter: ""
              pulltype: ""
            - name: "kata-qemu-tdx"
              snapshotter: ""
              pulltype: ""
            - name: "kata-qemu-snp"
              snapshotter: ""
              pulltype: ""
          - op: add
            path: /spec/config/defaultRuntimeClassName
            value: "kata-qemu"
          - op: add
            path: /spec/config/debug
            value: true
          - op: add
            path: /spec/config/environmentVariables
            value:
              - name: NODE_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: spec.nodeName
              - name: DEBUG
                value: "false"
              - name: SHIMS
                value: "clh cloud-hypervisor dragonball fc qemu qemu-coco-dev qemu-runtime-rs qemu-se-runtime-rs qemu-sev qemu-snp qemu-tdx stratovirt qemu-nvidia-gpu qemu-nvidia-gpu-snp qemu-nvidia-gpu-tdx"
              - name: DEFAULT_SHIM
                value: "qemu"
              - name: CREATE_RUNTIMECLASSES
                value: "true"
              - name: CREATE_DEFAULT_RUNTIMECLASS
                value: "false"
              - name: ALLOWED_HYPERVISOR_ANNOTATIONS
                value: ""
              - name: SNAPSHOTTER_HANDLER_MAPPING
                value: ""
              - name: AGENT_HTTPS_PROXY
                value: ""
              - name: AGENT_NO_PROXY
                value: ""
              - name: PULL_TYPE_MAPPING
                value: ""
              - name: INSTALLATION_PREFIX
                value: ""
              - name: MULTI_INSTALL_SUFFIX
                value: ""
              # Don't install containerd for RKE2-managed clusters
              - name: "INSTALL_OFFICIAL_CONTAINERD"
                value: "false"
              - name: "INSTALL_COCO_CONTAINERD"
                value: "false"
              - name: "INSTALL_VFIO_GPU_CONTAINERD"
                value: "false"
              # Disable nydus for RKE2
              - name: "INSTALL_NYDUS_SNAPSHOTTER"
                value: "false"
              # RKE2-specific configurations
              - name: "RKE2_DEPLOYMENT"
                value: "true"
              # Auto-detect containerd socket location
              - name: "AUTO_DETECT_CONTAINERD"
                value: "true"
          - op: replace
            path: /spec/config/installerVolumes
            value:
              # RKE2 paths only
              - hostPath:
                  path: /var/lib/rancher/rke2/agent/etc/containerd/
                  type: DirectoryOrCreate
                name: containerd-conf
              - hostPath:
                  path: /
                  type: ""
                name: host
          - op: replace
            path: /spec/config/installerVolumeMounts
            value:
              - mountPath: /etc/containerd/
                name: containerd-conf
              - mountPath: /host/
                name: host
          - op: add
            path: /spec/ccNodeSelector
            value:
              matchExpressions:
              - key: node-role.kubernetes.io/worker
                operator: In
                values:
                - "true"
              - key: node-role.kubernetes.io/etcd
                operator: NotIn
                values:
                - "true"
              - key: node-role.kubernetes.io/controlplane
                operator: NotIn
                values:
                - "true"
        target:
          kind: CcRuntime
    dest: "{{ ansible_env.HOME }}/operator/config/samples/ccruntime/rancher/kustomization.yaml"
  when: inventory_hostname in groups['master_nodes']
  become: true

- name: Apply Rancher customization
  shell: |
    cd {{ ansible_env.HOME }}/operator/config/samples/ccruntime/rancher
    kubectl apply -k .
  args:
    executable: /bin/bash
  when: inventory_hostname in groups['master_nodes']

# Wait for operator installation to complete
- name: Wait for cc-operator-pre-install-daemon to be ready
  shell: kubectl wait -n confidential-containers-system --for=condition=ready --selector=name=cc-operator-pre-install-daemon --timeout=10m pod
  args:
    executable: /bin/bash
  when: inventory_hostname in groups['master_nodes']
  register: pre_install_ready
  retries: 3
  delay: 10

- name: Wait for cc-operator-daemon-install to be ready
  shell: kubectl wait -n confidential-containers-system --for=condition=ready --selector=name=cc-operator-daemon-install --timeout=10m pod
  args:
    executable: /bin/bash
  when: inventory_hostname in groups['master_nodes']
  register: daemon_install_ready
  retries: 3
  delay: 10

- name: Check if node labeling is complete
  shell: |
    kubectl logs -n confidential-containers-system -l name=cc-operator-daemon-install --tail=50 | grep -q "node/.* labeled" && echo "Node labeled successfully"
  args:
    executable: /bin/bash
  when: inventory_hostname in groups['master_nodes']
  register: node_labeled
  until: node_labeled.rc == 0
  retries: 30
  delay: 10

- name: Verify runtime classes are created
  shell: kubectl get runtimeclass | grep kata-qemu-tdx
  args:
    executable: /bin/bash
  register: runtime_check
  when: inventory_hostname in groups['master_nodes']
  until: runtime_check.rc == 0
  retries: 10
  delay: 5

# Restart RKE services
- name: Restart rke2-server on master nodes only
  systemd:
    name: rke2-server
    state: restarted
  when:
    - inventory_hostname in groups['master_nodes'] | default([]) or (groups['all'] | length == 1)
  become: true
  ignore_errors: yes

- name: Restart rke2-agent on dedicated worker nodes only
  systemd:
    name: rke2-agent
    state: restarted
  when:
    - inventory_hostname in groups['worker_nodes'] | default([])
    - inventory_hostname not in groups['master_nodes'] | default([])
    - groups['all'] | length > 1 # Not a single-node deployment
  become: true
  ignore_errors: yes

# Setup Attestation (if enabled, master nodes only)
- name: Set up attestation components
  block:
    - name: Clone Confidential Containers Trustee repository
      git:
        repo: https://github.com/confidential-containers/trustee
        dest: "{{ ansible_env.HOME }}/trustee"
        version: "{{ coco.attestation.version }}"

    - name: Update attestation configuration
      shell: |
        cd {{ ansible_env.HOME }}/trustee/kbs/config/kubernetes/
        sed -i 's/built-in-as-v0.11.0/built-in-as-v0.12.0/g' base/kustomization.yaml
        sed -i 's/:built-in-as-v0.10.1//g' ita/kustomization.yaml
        sed -i 's/ita-as-v0.10.1/ita-as-v0.12.0/g' ita/kustomization.yaml

    # Generate certificates required for attestation
    - name: Create directory for certificates if it doesn't exist
      file:
        path: "{{ ansible_env.HOME }}/trustee/kbs/config/kubernetes/base"
        state: directory
        mode: "0755"

    - name: Check if kbs.pem exists
      stat:
        path: "{{ ansible_env.HOME }}/trustee/kbs/config/kubernetes/base/kbs.pem"
      register: kbs_cert_file

    - name: Generate ED25519 key for KBS
      shell: |
        cd {{ ansible_env.HOME }}/trustee/kbs/config/kubernetes/base
        openssl genpkey -algorithm ed25519 > kbs.key
        openssl pkey -in kbs.key -pubout -out kbs.pem
      when: not kbs_cert_file.stat.exists
      become: true

    # Set up secret for KBS
    - name: Create overlays directory
      file:
        path: "{{ ansible_env.HOME }}/trustee/kbs/config/kubernetes/overlays"
        state: directory
        mode: "0755"
      become: true

    - name: Create KBS secret from environment variable or default
      shell: |
        cd {{ ansible_env.HOME }}/trustee/kbs/config/kubernetes
        echo "${KBS_SECRET:-This is my super secret}" > overlays/key.bin
      environment:
        KBS_SECRET: "{{ lookup('env', 'KBS_SECRET') | default('This is my super secret') }}"
      become: true

    - name: Apply DCAP-based attestation service
      shell: |
        cd {{ ansible_env.HOME }}/trustee/kbs/config/kubernetes/
        export DEPLOYMENT_DIR=custom_pccs
        kubectl apply -k ./base
      args:
        executable: /bin/bash
      when: not coco.kbs.ita_enabled
      environment:
        DEPLOYMENT_DIR: custom_pccs

    - name: Apply Intel Trust Authority-based attestation (if enabled)
      shell: |
        cd {{ ansible_env.HOME }}/trustee/kbs/config/kubernetes/
        kubectl apply -k ./ita
      args:
        executable: /bin/bash
      when: coco.kbs.ita_enabled

    # Wait for KBS to be ready
    - name: Wait for KBS deployment to be ready
      shell: kubectl wait -n coco-tenant --for=condition=ready --timeout=5m pod -l app=kbs
      args:
        executable: /bin/bash
      register: kbs_ready
      retries: 3
      delay: 10

    - name: Get KBS service address
      shell: |
        export KBS_ADDRESS=http://$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[0].address}'):$(kubectl get svc kbs -n coco-tenant -o jsonpath='{.spec.ports[0].nodePort}')
        echo $KBS_ADDRESS
      args:
        executable: /bin/bash
      register: kbs_address

    - name: Display KBS service information
      debug:
        msg: |
          KBS Service is running!
          KBS Address: {{ kbs_address.stdout }}

          To use KBS, export this address:
          export KBS_ADDRESS={{ kbs_address.stdout }}

  when: inventory_hostname in groups['master_nodes'] and coco.attestation.enabled

# Verify setup
- name: Display runtime class check results
  debug:
    var: runtime_check.stdout
  when: inventory_hostname in groups['master_nodes']

# Add this section after the CoCo operator installation and before the "Verify setup" section

# Install Kyverno for policy enforcement
- name: Install Kyverno
  block:
    - name: Apply Kyverno installation manifest
      shell: kubectl create -f https://github.com/kyverno/kyverno/releases/download/v1.11.1/install.yaml
      args:
        executable: /bin/bash
      register: kyverno_install
      failed_when: 
        - kyverno_install.rc != 0
        - '"already exists" not in kyverno_install.stderr'
      changed_when: kyverno_install.rc == 0

    - name: Wait for Kyverno pods to be ready
      shell: kubectl wait --for=condition=ready -n kyverno pod -l app.kubernetes.io/part-of=kyverno --timeout=300s
      args:
        executable: /bin/bash
      retries: 3
      delay: 10

    - name: Verify Kyverno is running
      shell: kubectl get pods -n kyverno
      args:
        executable: /bin/bash
      register: kyverno_pods

    - name: Display Kyverno pods status
      debug:
        var: kyverno_pods.stdout_lines

  when: inventory_hostname in groups['master_nodes']
  tags:
    - kyverno

# Configure Kyverno policy for locked TDX deployments
- name: Configure TDX deployment lock policy
  block:
    - name: Create Kyverno policy directory
      file:
        path: "{{ ansible_env.HOME }}/kyverno-policies"
        state: directory
        mode: "0755"

    - name: Create TDX deployment lock policy
      copy:
        dest: "{{ ansible_env.HOME }}/kyverno-policies/tdx-deployment-lock-policy.yaml"
        content: |
          {% raw %}
          apiVersion: kyverno.io/v1
          kind: ClusterPolicy
          metadata:
            name: deny-updates-to-locked-tdx-deployments
          spec:
            validationFailureAction: Enforce
            background: true
            rules:
              - name: deny-update-if-locked
                match:
                  any:
                  - resources:
                      kinds:
                        - Deployment
                      namespaces:
                        - aether-5gc
                      operations:
                        - UPDATE
                validate:
                  message: "This Deployment is locked with TDX protection. Unlock it by removing the 'intel.com/locked' annotation."
                  deny:
                    conditions:
                      all:
                        - key: "{{ request.object.metadata.annotations.\"intel.com/locked\" || '' }}"
                          operator: Equals
                          value: "true"
          {% endraw %}

    - name: Apply TDX deployment lock policy
      shell: kubectl apply -f {{ ansible_env.HOME }}/kyverno-policies/tdx-deployment-lock-policy.yaml
      args:
        executable: /bin/bash
      register: policy_apply

    - name: Verify policy is active
      shell: kubectl get clusterpolicy deny-updates-to-locked-tdx-deployments
      args:
        executable: /bin/bash
      register: policy_status

    - name: Display policy status
      debug:
        msg: |
          TDX Deployment Lock Policy Status:
          {{ policy_status.stdout }}

  when: inventory_hostname in groups['master_nodes']
  tags:
    - kyverno
    - kyverno-policy
    
# Apply TDX runtime and lock to specific services
- name: Configure TDX for 5G services
  block:
    - name: Wait for deployments to be ready
      shell: |
        kubectl wait --for=condition=available --timeout=300s deployment --all -n aether-5gc || true
      args:
        executable: /bin/bash

    - name: Copy TDX service configuration script
      copy:
        content: |
          #!/bin/bash
          set -e  # Exit on error
          
          NAMESPACE="aether-5gc"
          RUNTIME_CLASS="kata-qemu"  # Using TDX-specific runtime
          LOCK_ANNOTATION_KEY="intel.com/locked"
          SERVICES=("nrf" "pcf" "amf" "smf")
          
          echo "🚀 Starting TDX configuration for 5G services..."
          
          # Function to check if deployment exists
          deployment_exists() {
              kubectl get deployment "$1" -n "$NAMESPACE" &>/dev/null
          }
          
          # Function to apply runtime class
          apply_runtime_class() {
              local deploy=$1
              echo "   🔧 Applying runtime class to $deploy..."
              
              # Get current runtime class
              CURRENT_RUNTIME=$(kubectl get deployment "$deploy" -n "$NAMESPACE" -o jsonpath='{.spec.template.spec.runtimeClassName}' 2>/dev/null || echo "none")
              echo "   Current runtime class: ${CURRENT_RUNTIME:-none}"
              
              # Apply runtime class
              if kubectl patch deployment "$deploy" -n "$NAMESPACE" --type='strategic' -p "{\"spec\":{\"template\":{\"spec\":{\"runtimeClassName\":\"$RUNTIME_CLASS\"}}}}"; then
                  echo "   ✅ Runtime class applied successfully"
                  return 0
              else
                  echo "   ❌ Failed to apply runtime class"
                  return 1
              fi
          }
          
          # Function to apply lock annotation
          apply_lock_annotation() {
              local deploy=$1
              echo "   🔒 Applying lock annotation to $deploy..."
              
              # Apply the annotation
              if kubectl annotate deployment "$deploy" -n "$NAMESPACE" "$LOCK_ANNOTATION_KEY=true" --overwrite; then
                  # Verify it was applied
                  sleep 2  # Small delay to ensure annotation is persisted
                  local applied_value=$(kubectl get deployment "$deploy" -n "$NAMESPACE" -o jsonpath="{.metadata.annotations.intel\.com/locked}" 2>/dev/null)
                  if [ "$applied_value" == "true" ]; then
                      echo "   ✅ Lock annotation verified: $applied_value"
                      return 0
                  else
                      echo "   ❌ Lock annotation not verified (value: ${applied_value:-none})"
                      return 1
                  fi
              else
                  echo "   ❌ Failed to apply lock annotation"
                  return 1
              fi
          }
          
          # Main processing loop
          FAILED_SERVICES=()
          SUCCESS_SERVICES=()
          
          for svc in "${SERVICES[@]}"; do
              echo ""
              echo "📦 Processing $svc deployment..."
              
              if ! deployment_exists "$svc"; then
                  echo "   ⚠️  Deployment $svc not found in namespace $NAMESPACE"
                  FAILED_SERVICES+=("$svc (not found)")
                  continue
              fi
              
              # Apply runtime class
              if ! apply_runtime_class "$svc"; then
                  FAILED_SERVICES+=("$svc (runtime)")
                  continue
              fi
              
              # Apply lock annotation
              if ! apply_lock_annotation "$svc"; then
                  FAILED_SERVICES+=("$svc (annotation)")
                  continue
              fi
              
              # Restart deployment
              echo "   🔁 Restarting $svc deployment..."
              kubectl rollout restart deployment "$svc" -n "$NAMESPACE"
              
              SUCCESS_SERVICES+=("$svc")
          done
          
          # Wait for all rollouts to complete
          echo ""
          echo "⏳ Waiting for rollouts to complete..."
          for svc in "${SUCCESS_SERVICES[@]}"; do
              echo "   Waiting for $svc..."
              kubectl rollout status deployment "$svc" -n "$NAMESPACE" --timeout=300s || echo "   ⚠️  Rollout timeout for $svc"
          done
          
          # Final verification with proper escaping
          echo ""
          echo "📊 Final deployment status:"
          kubectl get deployments -n "$NAMESPACE" -o custom-columns='NAME:.metadata.name,LOCKED:.metadata.annotations.intel\.com/locked,RUNTIME:.spec.template.spec.runtimeClassName'
          
          # Double-check annotations are actually there
          echo ""
          echo "🔍 Verifying annotations in detail:"
          for svc in "${SERVICES[@]}"; do
              echo -n "   $svc: "
              kubectl get deployment "$svc" -n "$NAMESPACE" -o jsonpath='{.metadata.annotations}' | jq -r '."intel.com/locked" // "not set"' 2>/dev/null || echo "error checking"
          done
          
          # Summary
          echo ""
          echo "🎯 TDX configuration summary:"
          echo "   ✅ Successfully configured: ${#SUCCESS_SERVICES[@]} services"
          if [ ${#SUCCESS_SERVICES[@]} -gt 0 ]; then
              printf '      - %s\n' "${SUCCESS_SERVICES[@]}"
          fi
          
          if [ ${#FAILED_SERVICES[@]} -gt 0 ]; then
              echo "   ❌ Failed: ${#FAILED_SERVICES[@]} services"
              printf '      - %s\n' "${FAILED_SERVICES[@]}"
              exit 1
          fi
          
          echo ""
          echo "✅ All services configured successfully!"
        dest: "{{ ansible_env.HOME }}/configure-tdx-services.sh"
        mode: "0755"

    - name: Execute TDX service configuration
      shell: "{{ ansible_env.HOME }}/configure-tdx-services.sh"
      args:
        executable: /bin/bash
      register: tdx_config_output

    - name: Display TDX configuration results
      debug:
        var: tdx_config_output.stdout_lines

    - name: Verify lock annotations are present (additional check)
      shell: |
        echo "=== Verifying lock annotations ==="
        for svc in nrf pcf amf smf; do
          LOCKED=$(kubectl get deployment $svc -n aether-5gc -o jsonpath='{.metadata.annotations.intel\.com/locked}' 2>/dev/null)
          if [ "$LOCKED" == "true" ]; then
            echo "✅ $svc is locked"
          else
            echo "❌ $svc is NOT locked (value: ${LOCKED:-none})"
          fi
        done
      args:
        executable: /bin/bash
      register: lock_verify_output

    - name: Display lock verification
      debug:
        var: lock_verify_output.stdout_lines

  when: 
    - inventory_hostname in groups['master_nodes']
    - coco.configure_5g_services | default(true)
  tags:
    - tdx-services
    - 5g-config

# Cleanup handler for failed deployments
- name: Add cleanup instructions
  debug:
    msg: |
      If deployment fails, run the playbook with cleanup tag:
      ansible-playbook -i inventory playbook.yml --tags cleanup

      Or manually cleanup with:
      sudo rm -rf {{ ansible_env.HOME }}/tdx
      sudo rm -rf {{ ansible_env.HOME }}/operator
      sudo rm -rf {{ ansible_env.HOME }}/trustee
      kubectl delete -k {{ ansible_env.HOME }}/operator/config/default || true
      kubectl delete namespace confidential-containers-system || true
